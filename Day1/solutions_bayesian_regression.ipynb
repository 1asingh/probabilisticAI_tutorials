{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">This notebook is an adapted version from  </span>  http://pyro.ai/examples/bayesian_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's begin by importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Uniform, Delta, Gamma, Binomial\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.optim as optim\n",
    "\n",
    "# for CI testing\n",
    "assert pyro.__version__.startswith('0.3.2')\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "\n",
    "The following example is taken from \\[1\\].  We would like to explore the relationship between topographic heterogeneity of a nation as measured by the Terrain Ruggedness Index (variable *rugged* in the dataset) and its GDP per capita. In particular, it was noted by the authors in \\[1\\] that terrain ruggedness or bad geography is related to poorer economic performance outside of Africa, but rugged terrains have had a reverse effect on income for African nations. Let us look at the data \\[2\\] and investigate this relationship.  We will be focusing on three features from the dataset:\n",
    "  - `rugged`: quantifies the Terrain Ruggedness Index\n",
    "  - `cont_africa`: whether the given nation is in Africa\n",
    "  - `rgdppc_2000`: Real GDP per capita for the year 2000\n",
    " \n",
    "  \n",
    "We will take the logarithm for the response variable GDP as it tends to vary exponentially. We also use a new variable `african_rugged`, defined as the product between the variables `rugged` and `cont_africa`, to capture the correlation between ruggedness and whether a country is in Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://d2fefpcigoriu7.cloudfront.net/datasets/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])\n",
    "df[\"african_rugged\"] = data[\"cont_africa\"] * data[\"rugged\"]\n",
    "df = df[[\"cont_africa\", \"rugged\", \"african_rugged\", \"rgdppc_2000\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 entries \n",
    "display(df[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning. Suppose we're given a dataset $\\mathcal{D}$ of the form\n",
    "\n",
    "$$ \\mathcal{D}  = \\{ (X_i, y_i) \\} \\qquad \\text{for}\\qquad i=1,2,...,N$$\n",
    "\n",
    "The goal of linear regression is to fit a function to the data of the form:\n",
    "\n",
    "$$ y = w X + b + \\epsilon $$\n",
    "\n",
    "where $w$ and $b$ are learnable parameters and $\\epsilon$ represents observation noise. Specifically $w$ is a matrix of weights and $b$ is a bias vector.\n",
    "\n",
    "Let's first implement linear regression in PyTorch and learn point estimates for the parameters $w$ and $b$.  Then we'll see how to incorporate uncertainty into our estimates by using Pyro to implement Bayesian regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "african_nations = data[data[\"cont_africa\"] == 1]\n",
    "non_african_nations = data[data[\"cont_africa\"] == 0]\n",
    "sns.scatterplot(non_african_nations[\"rugged\"], \n",
    "            np.log(non_african_nations[\"rgdppc_2000\"]), \n",
    "            ax=ax[0])\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "sns.scatterplot(african_nations[\"rugged\"], \n",
    "            np.log(african_nations[\"rgdppc_2000\"]), \n",
    "            ax=ax[1])\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Model\n",
    "We would like to predict log GDP per capita of a nation as a function of three features from the dataset - whether the nation is in Africa, its Terrain Ruggedness Index, and the interaction between these two.  Let's define our regression model. We'll define an specific object encapsulating this linear regression model.  Our input `x_data` is a tensor of size $N \\times 3$ and our output `y_data` is a tensor of size $N \\times 1$.  The method `predict(self,x_data)` defines a linear transformation of the form $Xw + b$ where $w$ is the weight matrix and $b$ is the additive bias.\n",
    "\n",
    "The parameters of the model are defined using ``torch.nn.Parameter``, and will be learned during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel():\n",
    "    def __init__(self):\n",
    "        self.w = torch.nn.Parameter(torch.zeros(1, 3))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(1, 1))\n",
    "\n",
    "    def params(self):\n",
    "        return {\"b\":self.b, \"w\": self.w}\n",
    "\n",
    "    def predict(self, x_data):\n",
    "        return (self.b + torch.mm(self.w, torch.t(x_data))).squeeze(0)\n",
    "\n",
    "regression_model = RegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training\n",
    "We will use the mean squared error (MSE) as our loss and Adam as our optimizer. We would like to optimize the parameters of the `regression_model` neural net above. We will use a somewhat large learning rate of `0.05` and run for 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(regression_model.params().values(), lr=0.05)\n",
    "num_iterations = 5000\n",
    "data = torch.tensor(df.values, dtype=torch.float)\n",
    "x_data, y_data = data[:, :-1], data[:, -1]\n",
    "\n",
    "def main():\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model.predict(x_data)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 500 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.params().items():\n",
    "        print(name, param.data.numpy())\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the regression line learned for african and non-afrian nations relating the rugeedness index with the GDP of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Regression line \", fontsize=16)\n",
    "ax[0].scatter(x_data[x_data[:,0]==0,1].detach().numpy(), y_data[x_data[:,0]==0].detach().numpy())\n",
    "ax[1].scatter(x_data[x_data[:,0]==1,1].detach().numpy(), y_data[x_data[:,0]==1].detach().numpy())\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regression_model.predict(x_data[x_data[:,0]==0,:]).detach().numpy(), color='r')\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regression_model.predict(x_data[x_data[:,0]==1,:]).detach().numpy(), color='r')\n",
    "\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"Non African Nations\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"African Nations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 The relationship between ruggedness and log GPD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this analysis, we can estimate the relationship between ruggedness and log GPD. As can be seen, this relationship is positive for African nations, but negative for Non African Nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_within_africa = regression_model.params()['w'][0,1] + regression_model.params()['w'][0,2]\n",
    "slope_outside_africa = regression_model.params()['w'][0,1]\n",
    "print(slope_within_africa)\n",
    "print(slope_outside_africa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bayesian Linear Regression\n",
    "\n",
    "\n",
    "[Bayesian modeling](http://mlg.eng.cam.ac.uk/zoubin/papers/NatureReprint15.pdf) offers a systematic framework for reasoning about model uncertainty. Instead of just learning point estimates, we're going to learn a _distribution_ over variables that are consistent with the observed data.\n",
    "\n",
    "In order to make our linear regression Bayesian, we need to put priors on the parameters $w$, $b$ and $f$. These are distributions that represent our prior belief about reasonable values for $w$ and $b$ (before observing any data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model\n",
    "\n",
    "We now have all the ingredients needed to specify our model. First we define priors over weights and bias. Note the priors that we are using for the different latent variables in the model. The prior on the intercept parameter is very flat as we would like this to be learnt from the data. We are using a weakly regularizing prior on the regression coefficients to avoid overfitting to the data.\n",
    "\n",
    "We use the `obs` argument to the `pyro.sample` statement to condition on the observed data `y_data` with a learned observation noise `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    # weight and bias priors\n",
    "    w = pyro.sample(\"w\", Normal(torch.zeros(1, 3), torch.ones(1, 3)).to_event(1))\n",
    "    b = pyro.sample(\"b\", Normal(0., 1000.))\n",
    "\n",
    "    precision = pyro.sample(\"precision\", Gamma(1., 1.))\n",
    "    with pyro.plate(\"map\", len(x_data)):\n",
    "        # run the nn forward on data\n",
    "        prediction_mean = (b + torch.mm(x_data,torch.t(w))).squeeze(-1)\n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\", Normal(prediction_mean, torch.sqrt(1./precision)), obs=y_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Guide\n",
    "\n",
    "In order to do inference we're going to need a guide, i.e. a variational family of distributions.  We will use Pyro's [autoguide library](http://docs.pyro.ai/en/dev/contrib.autoguide.html) to automatically place Gaussians with diagonal covariance on all of the distributions in the model.  Under the hood, this defines a `guide` function with `Normal` distributions with learnable parameters corresponding to each `sample()` in the model.  Autoguide also supports learning MAP estimates with `AutoDelta` or composing guides with `AutoGuideList` (see the [docs](http://docs.pyro.ai/en/dev/contrib.autoguide.html) for more information). In Day 3 we will explore how to write guides by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Inference\n",
    "\n",
    "To do inference we'll use stochastic variational inference (SVI) (for an introduction to SVI, see [SVI Part I](svi_part_i.ipynb)). Just like in the non-Bayesian linear regression, each iteration of our training loop will take a gradient step, with the difference that in this case, we'll use the ELBO objective instead of the MSE loss by constructing a `Trace_ELBO` object that we pass to `SVI`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.1})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO(), num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `Adam` is a thin wrapper around `torch.optim.Adam` (see [here](svi_part_i.ipynb#Optimizers) for a discussion). To take an ELBO gradient step we simply call the step method of SVI. Notice that the data argument we pass to step will be passed to both model() and guide().  The complete training loop is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 5000\n",
    "def train(x_data, y_data):\n",
    "    pyro.clear_param_store()\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(x_data, y_data)\n",
    "        if j % 500 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))\n",
    "\n",
    "train(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, instead of just point estimates, we now have uncertainty estimates (`auto_scale`) for our learned parameters.  Note that Autoguide packs the latent variables into a tensor, in this case, one entry per variable sampled in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Model Evaluation I: Prediction's Uncertainty\n",
    "\n",
    "We first show how the model is able to capture uncertainty in their predictions by using the ``precision`` random variable which models the variance in the outputs of the linear regression model. Using this information we can compute ``prediction intervals`` (i.e. [mean-2\\*stdev,mean+2\\*stdev]) for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionLineMean(x_data,guide):\n",
    "    return (pyro.param('auto_loc')[3] + torch.mm(x_data,torch.t(pyro.param('auto_loc')[0:3].reshape(1,3)))).squeeze(-1)\n",
    "\n",
    "\n",
    "def regressionLineMeanCI(x_data,guide):\n",
    "    return [(pyro.param('auto_loc')[3] + torch.mm(x_data,torch.t(pyro.param('auto_loc')[0:3].reshape(1,3)))).squeeze(-1) -2*torch.sqrt(1./torch.nn.Softplus()(pyro.param('auto_loc')[4])), \n",
    "            (pyro.param('auto_loc')[3] + torch.mm(x_data,torch.t(pyro.param('auto_loc')[0:3].reshape(1,3)))).squeeze(-1) +2*torch.sqrt(1./torch.nn.Softplus()(pyro.param('auto_loc')[4]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Uncertainty in Data Prediction\", fontsize=16)\n",
    "ax[0].scatter(x_data[x_data[:,0]==0,1].detach().numpy(), y_data[x_data[:,0]==0].detach().numpy())\n",
    "ax[1].scatter(x_data[x_data[:,0]==1,1].detach().numpy(), y_data[x_data[:,0]==1].detach().numpy())\n",
    "\n",
    "ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regressionLineMean(x_data[x_data[:,0]==0,:],guide).detach().numpy(), color='r', label = 'Regression Line')\n",
    "ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regressionLineMeanCI(x_data[x_data[:,0]==0,:],guide)[0].detach().numpy(), color='lightgrey', label = 'Regression 95%-PI')\n",
    "ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regressionLineMeanCI(x_data[x_data[:,0]==0,:],guide)[1].detach().numpy(), color='lightgrey')\n",
    "ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regressionLineMean(x_data[x_data[:,0]==1,:],guide).detach().numpy(), color='r', label = 'Regression Line')\n",
    "ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regressionLineMeanCI(x_data[x_data[:,0]==1,:],guide)[0].detach().numpy(), color='lightgrey', label = 'Regression 95%-PI')\n",
    "ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regressionLineMeanCI(x_data[x_data[:,0]==1,:],guide)[1].detach().numpy(), color='lightgrey')\n",
    "\n",
    "\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"Non African Nations\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"African Nations\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look like all the data points fall inside the estimated confidence intervals, being able to capture the uncertaintity in our predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Model Evaluation II: Model's Uncertainty\n",
    "In this case, we'll sample different the regression lines to see how using a Bayesian approach we are able to capture the model undertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionLineSample(x_data,guide):\n",
    "    return (guide()['b'] + torch.mm(x_data,torch.t(guide()['w']))).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Uncertainty in Regression line \", fontsize=16)\n",
    "ax[0].scatter(x_data[x_data[:,0]==0,1].detach().numpy(), y_data[x_data[:,0]==0].detach().numpy())\n",
    "ax[1].scatter(x_data[x_data[:,0]==1,1].detach().numpy(), y_data[x_data[:,0]==1].detach().numpy())\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regressionLineSample(x_data[x_data[:,0]==0,:],guide).detach().numpy(), color='r')\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regressionLineSample(x_data[x_data[:,0]==1,:],guide).detach().numpy(), color='r')\n",
    "\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"Non African Nations\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"African Nations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the uncertainty in our estimate of the regression line. Note that for lower values of ruggedness there are many more data points, and as such, the regression lines are less uncertainty than in high ruggness values, where there is much more uncertainty, specially in the case of African nations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Model Evaluation III: Combining uncertainties\n",
    "\n",
    "Finally we can combine uncertainties in the model with uncertainties in the predictions, given rise to a more accurate modelling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressionLineSampleCI(x_data,guide):\n",
    "    return [(guide()['b'] + torch.mm(x_data,torch.t(guide()['w']))).squeeze(-1) -2/torch.sqrt(guide()['precision']), \n",
    "            (guide()['b'] + torch.mm(x_data,torch.t(guide()['w']))).squeeze(-1) +2/torch.sqrt(guide()['precision'])]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Combined Uncertainty in Regression line and Predictions\", fontsize=16)\n",
    "ax[0].scatter(x_data[x_data[:,0]==0,1].detach().numpy(), y_data[x_data[:,0]==0].detach().numpy())\n",
    "ax[1].scatter(x_data[x_data[:,0]==1,1].detach().numpy(), y_data[x_data[:,0]==1].detach().numpy())\n",
    "\n",
    "for i in range(10):\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regressionLineSample(x_data[x_data[:,0]==0,:],guide).detach().numpy(), color='r')\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regressionLineSampleCI(x_data[x_data[:,0]==0,:],guide)[0].detach().numpy(), color='lightgrey')\n",
    "    ax[0].plot(x_data[x_data[:,0]==0,1].detach().numpy(),regressionLineSampleCI(x_data[x_data[:,0]==0,:],guide)[1].detach().numpy(), color='lightgrey')\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regressionLineSample(x_data[x_data[:,0]==1,:],guide).detach().numpy(), color='r')\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regressionLineSampleCI(x_data[x_data[:,0]==1,:],guide)[0].detach().numpy(), color='lightgrey')\n",
    "    ax[1].plot(x_data[x_data[:,0]==1,1].detach().numpy(),regressionLineSampleCI(x_data[x_data[:,0]==1,:],guide)[1].detach().numpy(), color='lightgrey')\n",
    "\n",
    "\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"Non African Nations\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",ylabel=\"log GDP (2000)\",title=\"African Nations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 The relationship between ruggedness and log GPD\n",
    "\n",
    "Finally, we go back to the previous analysis about the relationship between ruggedness and log GPD. Now, we can compute uncertaintis obver this relationship. As can be seen, this relationship is negative for Non African Nations with high probability, and positive for African nations in most of the cases. In fact, there is non-negligible probability that this is relationship is also negative. This is the consequence of the low number of samples in the case of African nations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.stack([guide()['w'] for i in range(1000)]).squeeze()\n",
    "gamma_within_africa = weight[:,1] + weight[:,2]\n",
    "gamma_outside_africa = weight[:,1]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.distplot(gamma_within_africa.detach().numpy(), kde_kws={\"label\": \"African nations\"},)\n",
    "sns.distplot(gamma_outside_africa.detach().numpy(), kde_kws={\"label\": \"Non-African nations\"})\n",
    "fig.suptitle(\"Density of Slope : log(GDP) vs. Terrain Ruggedness\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Exercise</span> \n",
    "\n",
    "Build a Bayesian logistic regression for predicting african vs non-african countries based on ruggedness index and log GDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 entries \n",
    "display(df[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = data[:, (1,3)], data[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    # weight and bias priors\n",
    "    w = pyro.sample(\"w\", Normal(torch.zeros(1, 2), torch.ones(1, 2)).to_event(1))\n",
    "    b = pyro.sample(\"b\", Normal(0., 1000.))\n",
    "\n",
    "    with pyro.plate(\"map\", len(x_data)):\n",
    "        # Compute logits as a linear combination between data and weights. \n",
    "        logits = (b + torch.mm(x_data,torch.t(w))).squeeze(-1)\n",
    "        # Define a Binomial distribution as the observed value \n",
    "        pyro.sample(\"african/non-african\", Binomial(logits = logits), obs=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.1})\n",
    "guide = AutoDiagonalNormal(model)\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO(), num_samples=10)\n",
    "train(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegressionLineMean(x_data,guide):\n",
    "    return (pyro.param('auto_loc')[2] + torch.mm(x_data,torch.t(pyro.param('auto_loc')[0:2].reshape(1,2)))).squeeze(-1)\n",
    "\n",
    "def logisticRegressionPredictions(x_data,guide):\n",
    "    logits = logisticRegressionLineMean(x_data,guide)\n",
    "    return Binomial(logits = logits).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "xx, yy = np.mgrid[0:7:.01, 6:11:.01]\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "probs = logisticRegressionPredictions(grid,guide).reshape(xx.shape).detach().numpy()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.contour(xx, yy, probs, levels=[.5], cmap=\"Reds\", vmin=0, vmax=.6)\n",
    "\n",
    "ax.scatter(x_data[y_data==0,0], x_data[y_data==0, 1], c='g', s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1, label='Non-African')\n",
    "\n",
    "ax.scatter(x_data[y_data==1,0], x_data[y_data==1, 1], c='orange', s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1, label='African')\n",
    "\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(0, 7), ylim=(6, 11),\n",
    "       xlabel=\"Rugged\", ylabel=\"Log GDP\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegressionLineSample(x_data,guide):\n",
    "    return (guide()['b'] + torch.mm(x_data,torch.t(guide()['w']))).squeeze(-1)\n",
    "\n",
    "def logisticRegressionPredictionsSample(x_data,guide):\n",
    "    logits = logisticRegressionLineSample(x_data,guide)\n",
    "    return Binomial(logits = logits).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "xx, yy = np.mgrid[0:7:.01, 6:11:.01]\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    probs = logisticRegressionPredictionsSample(grid,guide).reshape(xx.shape).detach().numpy()\n",
    "    ax.contour(xx, yy, probs, levels=[.5], cmap=\"Greys\", vmin=0, vmax=1.5)\n",
    "\n",
    "probs = logisticRegressionPredictions(grid,guide).reshape(xx.shape).detach().numpy()\n",
    "ax.contour(xx, yy, probs, levels=[.5], cmap=\"Reds\", vmin=0, vmax=.6)\n",
    "\n",
    "\n",
    "ax.scatter(x_data[y_data==0,0], x_data[y_data==0, 1], c='g', s=50,\n",
    "           cmap=\"Greens\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1, label='Non-African')\n",
    "\n",
    "ax.scatter(x_data[y_data==1,0], x_data[y_data==1, 1], c='orange', s=50,\n",
    "           cmap=\"Oranges\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1, label='African')\n",
    "\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(0, 7), ylim=(6, 11),\n",
    "       xlabel=\"Rugged\", ylabel=\"Log GDP\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "  1. McElreath, D., *Statistical Rethinking, Chapter 7*, 2016\n",
    "  2. Nunn, N. & Puga, D., *[Ruggedness: The blessing of bad geography in Africa\"](https://diegopuga.org/papers/rugged.pdf)*, Review of Economics and Statistics 94(1), Feb. 2012"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "probabilistic.ai",
   "language": "python",
   "name": "probabilistic.ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
